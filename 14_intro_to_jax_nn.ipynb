{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Introduction to JAX Neural Networks\n",
                "\n",
                "JAX doesn't have a built-in neural network library like PyTorch or TensorFlow, but we can build one using its primitives. Libraries like Flax, Haiku, and Equinox are built on top of JAX."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import jax\n",
                "import jax.numpy as jnp\n",
                "from jax import random"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Manual Linear Layer\n",
                "\n",
                "A linear layer `y = Wx + b` in JAX."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def linear(params, x):\n",
                "    w, b = params\n",
                "    return jnp.dot(x, w) + b\n",
                "\n",
                "key = random.PRNGKey(0)\n",
                "key_w, key_b = random.split(key)\n",
                "w = random.normal(key_w, (3, 2))\n",
                "b = random.normal(key_b, (2,))\n",
                "params = (w, b)\n",
                "\n",
                "x = jnp.ones((5, 3))\n",
                "preds = linear(params, x)\n",
                "print(\"Predictions shape:\", preds.shape)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Training Loop Concept\n",
                "\n",
                "In JAX, we explicitly manage state (params). We use `jax.grad` to get gradients and update params."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def loss_fn(params, x, y):\n",
                "    preds = linear(params, x)\n",
                "    return jnp.mean((preds - y) ** 2)\n",
                "\n",
                "# grad_fn = jax.grad(loss_fn)\n",
                "# This returns a function that computes gradients w.r.t the first argument (params)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}